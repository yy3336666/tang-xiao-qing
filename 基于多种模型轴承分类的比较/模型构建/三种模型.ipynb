{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MDB(nn.Module): \n",
    "    def __init__(self,input,hiden):  \n",
    "        super(MDB,self).__init__()\n",
    "        self.conv1=nn.Sequential(  \n",
    "            nn.Conv2d(in_channels = input,out_channels = hiden,kernel_size = 3 , stride = 1,padding=1),  \n",
    "            nn.BatchNorm2d(hiden),  \n",
    "            nn.ReLU(),  \n",
    "            nn.Conv2d(in_channels = hiden,out_channels = hiden,kernel_size = 3 , stride = 1,padding=2,dilation=2),  \n",
    "            nn.BatchNorm2d(hiden),   \n",
    "            nn.ReLU(),   \n",
    "            nn.Conv2d(in_channels = hiden,out_channels = hiden,kernel_size = 3 , stride = 1,padding=2,dilation=2),  \n",
    "            nn.BatchNorm2d(hiden),  \n",
    "\t        nn.ReLU()  \n",
    "      )  \n",
    "        self.conv2=nn.Sequential(  \n",
    "            nn.Conv2d(in_channels = input,out_channels = hiden,kernel_size = 5 , stride = 1,padding=2),  \n",
    "            nn.BatchNorm2d(hiden),  \n",
    "            nn.ReLU(),  \n",
    "            nn.Conv2d(in_channels = hiden,out_channels = hiden,kernel_size = 5 , stride = 1,padding=4,dilation=2),  \n",
    "            nn.BatchNorm2d(hiden),   \n",
    "            nn.ReLU(),   \n",
    "            nn.Conv2d(in_channels = hiden,out_channels =hiden,kernel_size = 5 , stride = 1,padding=4,dilation=2),  \n",
    "            nn.BatchNorm2d(hiden),  \n",
    "\t        nn.ReLU()  \n",
    "      )  \n",
    "        self.conv3=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input,out_channels=hiden,kernel_size=1,stride=1,padding=0),\n",
    "            nn.BatchNorm2d(hiden),  \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self ,x):\n",
    "        p1=self.conv1(x)\n",
    "        p2=self.conv2(x)\n",
    "        p3=self.conv3(x)\n",
    "        x=F.relu(torch.concat((p1,p2,p3),dim=1))\n",
    "        return x \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ChannelAttentionModule(nn.Module):\n",
    "    def __init__(self, channel, ratio=16):\n",
    "        super(ChannelAttentionModule, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.shared_MLP = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel // ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channel // ratio, channel, 1, bias=False)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avgout = self.shared_MLP(self.avg_pool(x))\n",
    "        maxout = self.shared_MLP(self.max_pool(x))\n",
    "        return self.sigmoid(avgout + maxout)\n",
    "\n",
    "\n",
    "class SpatialAttentionModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpatialAttentionModule, self).__init__()\n",
    "        self.conv2d = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=7, stride=1, padding=3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avgout = torch.mean(x, dim=1, keepdim=True)\n",
    "        maxout, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avgout, maxout], dim=1)\n",
    "        out = self.sigmoid(self.conv2d(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.channel_attention = ChannelAttentionModule(channel)\n",
    "        self.spatial_attention = SpatialAttentionModule()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.channel_attention(x) * x\n",
    "        out = self.spatial_attention(out) * out\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多尺度加注意力\n",
    "class MODEL(nn.Module):\n",
    "    def __init__(self,input,hiden,output):  \n",
    "        super(MODEL,self).__init__()\n",
    "        self.layer1=MDB(input,hiden)\n",
    "        self.layer2=MDB(input,hiden)\n",
    "        self.layer3=MDB(input,hiden)\n",
    "        self.attention1=CBAM(3*hiden)\n",
    "        self.attention2=CBAM(3*hiden)\n",
    "        self.attention3=CBAM(3*hiden)\n",
    "        self.output=nn.Linear(9*hiden,output)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "    def forward(self,x):\n",
    "        p1=self.layer1(x)\n",
    "        p2=self.layer2(x)\n",
    "        p3=self.layer3(x)\n",
    "        p1_a=self.attention1(p1)\n",
    "        p2_a=self.attention2(p2)\n",
    "        p3_a=self.attention3(p3)\n",
    "        x=F.relu(torch.concat((p1_a,p2_a,p3_a),dim=1))\n",
    "        x=F.adaptive_avg_pool2d(x,(1,1))\n",
    "        x=x.view(len(x),x.shape[1])\n",
    "        x=self.output(x)\n",
    "        x=self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "#cnn\n",
    "class Simple_CNN(nn.Module):\n",
    "    def __init__(self,inputs, class_num):\n",
    "        super(Simple_CNN, self).__init__()\n",
    "        self.class_num = class_num\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\t\t# input: 3,400,600\n",
    "                in_channels=inputs,\n",
    "                out_channels=8,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=8,\n",
    "                out_channels=16,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.AvgPool2d(2),  # 16,400,600 --> 16,200,300\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=16,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=8,\n",
    "                kernel_size=5,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.AvgPool2d(2),  # 8,200,300 --> 8,100,150\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(\n",
    "                in_channels=8,\n",
    "                out_channels=8,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.Conv2d(\n",
    "                in_channels=8,\n",
    "                out_channels=1,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.AvgPool2d(2),  \n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.line = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=16,\n",
    "                out_features=self.class_num\n",
    "            ),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = x.view(-1, 16)\n",
    "        y = self.line(x)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多尺度\n",
    "class MDB_class(nn.Module): \n",
    "    def __init__(self,input,hiden,classnum):  \n",
    "        super(MDB_class,self).__init__()\n",
    "        self.conv1=nn.Sequential(  \n",
    "            nn.Conv2d(in_channels = input,out_channels = hiden,kernel_size = 3 , stride = 1,padding=1),  \n",
    "            nn.BatchNorm2d(hiden),  \n",
    "            nn.ReLU(),  \n",
    "            nn.Conv2d(in_channels = hiden,out_channels = hiden,kernel_size = 3 , stride = 1,padding=2,dilation=2),  \n",
    "            nn.BatchNorm2d(hiden),   \n",
    "            nn.ReLU(),   \n",
    "            nn.Conv2d(in_channels = hiden,out_channels = hiden,kernel_size = 3 , stride = 1,padding=2,dilation=2),  \n",
    "            nn.BatchNorm2d(hiden),  \n",
    "\t        nn.ReLU()  \n",
    "      )  \n",
    "        self.conv2=nn.Sequential(  \n",
    "            nn.Conv2d(in_channels = input,out_channels = hiden,kernel_size = 5 , stride = 1,padding=2),  \n",
    "            nn.BatchNorm2d(hiden),  \n",
    "            nn.ReLU(),  \n",
    "            nn.Conv2d(in_channels = hiden,out_channels = hiden,kernel_size = 5 , stride = 1,padding=4,dilation=2),  \n",
    "            nn.BatchNorm2d(hiden),   \n",
    "            nn.ReLU(),   \n",
    "            nn.Conv2d(in_channels = hiden,out_channels =hiden,kernel_size = 5 , stride = 1,padding=4,dilation=2),  \n",
    "            nn.BatchNorm2d(hiden),  \n",
    "\t        nn.ReLU()  \n",
    "      )  \n",
    "        self.conv3=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input,out_channels=hiden,kernel_size=1,stride=1,padding=0),\n",
    "            nn.BatchNorm2d(hiden),  \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.line = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=3*hiden*32*32,\n",
    "                out_features=classnum\n",
    "            ),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self ,x,hiden):\n",
    "        p1=self.conv1(x)\n",
    "        p2=self.conv2(x)\n",
    "        p3=self.conv3(x)\n",
    "        x=F.relu(torch.concat((p1,p2,p3),dim=1))\n",
    "        x=x.view(-1,3*hiden*32*32)\n",
    "        x=self.line(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_f10=pd.read_csv('实作-1107\\数据\\F10.csv',header=None).dropna()\n",
    "data_f5=pd.read_csv('实作-1107\\数据\\F5.csv',header=None).dropna()\n",
    "data_f15=pd.read_csv('实作-1107\\数据\\F15.csv',header=None).dropna()\n",
    "data_normal=pd.read_csv('实作-1107\\数据\\F15.csv',header=None).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_normal_train=[]\n",
    "for i in range(30):\n",
    "    fre = np.array(data_f5.iloc[:,1])[i*1024:(i+1)*1024]\n",
    "    result = fre.reshape((32, 32))\n",
    "    result =255*(result - np.min(result)) / (np.max(result) - np.min(result))\n",
    "    data_normal_train.append(result)\n",
    "data_normal_train=np.array(data_normal_train).astype(int)\n",
    "for i  in  range (len(data_normal_train)):\n",
    "    plt.imshow(data_normal_train[i],cmap=\"gray\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f5_train=[]\n",
    "for i in range(30):\n",
    "    fre = np.array(data_f5.iloc[:,1])[i*1024:(i+1)*1024]\n",
    "    result = fre.reshape((32, 32))\n",
    "    result =255*(result - np.min(result)) / (np.max(result) - np.min(result))\n",
    "    data_f5_train.append(result)\n",
    "data_f5_train=np.array(data_f5_train).astype(int)\n",
    "for i  in  range (len(data_f5_train)):\n",
    "    plt.imshow(data_f5_train[i],cmap=\"gray\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f10_train=[]\n",
    "for i in range(30):\n",
    "    fre = np.array(data_f10.iloc[:,1])[i*1024:(i+1)*1024]\n",
    "    result = fre.reshape((32, 32))\n",
    "    result = 255*(result - np.min(result)) / (np.max(result) - np.min(result))\n",
    "    data_f10_train.append(result)\n",
    "data_f10_train=np.array(data_f10_train).astype(int)\n",
    "for i  in  range (len(data_f10_train)):\n",
    "    plt.imshow(data_f10_train[i],cmap=\"gray\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f15_train=[]\n",
    "for i in range(30):\n",
    "    fre = np.array(data_f15.iloc[:,1])[i*1024:(i+1)*1024]\n",
    "    result = fre.reshape((32, 32))\n",
    "    result = 255*(result - np.min(result)) / (np.max(result) - np.min(result))\n",
    "    data_f15_train.append(result)\n",
    "data_f15_train=np.array(data_f15_train).astype(int)\n",
    "for i  in  range (len(data_f15_train)):\n",
    "    plt.imshow(data_f15_train[i],cmap=\"gray\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_f15_train=torch.from_numpy(data_f15_train).float()\n",
    "data_f10_train=torch.from_numpy(data_f10_train).float()\n",
    "data_f5_train=torch.from_numpy(data_f5_train).float()\n",
    "data_normal_train=torch.from_numpy(data_normal_train).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=torch.concat((data_f5_train,data_f10_train,data_f15_train,data_normal_train),dim=0)\n",
    "data_train=data_train.reshape(120,1,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "labels=torch.concat((torch.tensor([0]*30),torch.tensor([1]*30),torch.tensor([2]*30),torch.tensor([3]*30)))\n",
    "data = TensorDataset(data_train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import SubsetRandomSampler\n",
    "batch_size = 12\n",
    "validation_split = 0.2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "dataset_size = len(data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(data, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train set  accuracy: 0.3125\n",
      " train set  accuracy: 0.3541666567325592\n",
      " train set  accuracy: 0.3854166567325592\n",
      " train set  accuracy: 0.4270833432674408\n",
      " train set  accuracy: 0.4479166567325592\n",
      " train set  accuracy: 0.4479166567325592\n",
      " train set  accuracy: 0.4375\n",
      " train set  accuracy: 0.4375\n",
      " train set  accuracy: 0.4375\n",
      " train set  accuracy: 0.46875\n",
      " train set  accuracy: 0.4791666567325592\n",
      " train set  accuracy: 0.5104166865348816\n",
      " train set  accuracy: 0.5208333134651184\n",
      " train set  accuracy: 0.5\n",
      " train set  accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "#卷积层加多尺度\n",
    "model=MODEL(1,64,4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-4)\n",
    "for j in range(15):\n",
    "    train_accuracy=0\n",
    "    model.train()\n",
    "    for train,labels in train_loader:\n",
    "        out=model(train)\n",
    "        pred=torch.argmax(out,dim=1)\n",
    "        loss = loss_fn(out,labels)\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        train_accuracy+=(pred== labels).sum()\n",
    "    print(\" train set  accuracy: {}\".format(train_accuracy/len(train_indices)))\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for test ,labels  in validation_loader: \n",
    "            outputs = model(test)\n",
    "            pred=torch.argmax(outputs,dim=1)\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            total_test_loss = total_test_loss + loss.item()\n",
    "            accuracy = (pred == labels).sum()\n",
    "            total_accuracy = total_accuracy + accuracy\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train set  accuracy: 0.25\n",
      " train set  accuracy: 0.3125\n",
      " train set  accuracy: 0.2916666567325592\n",
      " train set  accuracy: 0.28125\n",
      " train set  accuracy: 0.3020833432674408\n",
      " train set  accuracy: 0.3020833432674408\n",
      " train set  accuracy: 0.3125\n",
      " train set  accuracy: 0.3333333432674408\n",
      " train set  accuracy: 0.2916666567325592\n",
      " train set  accuracy: 0.34375\n",
      " train set  accuracy: 0.3541666567325592\n",
      " train set  accuracy: 0.3125\n",
      " train set  accuracy: 0.3229166567325592\n",
      " train set  accuracy: 0.34375\n",
      " train set  accuracy: 0.3333333432674408\n"
     ]
    }
   ],
   "source": [
    "#cnn\n",
    "model=Simple_CNN(1,4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-4)\n",
    "for j in range(15):\n",
    "    train_accuracy=0\n",
    "    model.train()\n",
    "    for train,labels in train_loader:\n",
    "        out=model(train)\n",
    "        pred=torch.argmax(out,dim=1)\n",
    "        loss = loss_fn(out,labels)\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        train_accuracy+=(pred== labels).sum()\n",
    "    print(\" train set  accuracy: {}\".format(train_accuracy/len(train_indices)))\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for test ,labels  in validation_loader: \n",
    "            outputs = model(test)\n",
    "            pred=torch.argmax(outputs,dim=1)\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            total_test_loss = total_test_loss + loss.item()\n",
    "            accuracy = (pred == labels).sum()\n",
    "            total_accuracy = total_accuracy + accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " train set  accuracy: 0.25\n",
      " train set  accuracy: 0.4583333432674408\n",
      " train set  accuracy: 0.4895833432674408\n",
      " train set  accuracy: 0.6041666865348816\n",
      " train set  accuracy: 0.6979166865348816\n",
      " train set  accuracy: 0.6979166865348816\n",
      " train set  accuracy: 0.71875\n",
      " train set  accuracy: 0.7708333134651184\n",
      " train set  accuracy: 0.78125\n",
      " train set  accuracy: 0.7916666865348816\n",
      " train set  accuracy: 0.7916666865348816\n",
      " train set  accuracy: 0.78125\n",
      " train set  accuracy: 0.7916666865348816\n",
      " train set  accuracy: 0.7916666865348816\n",
      " train set  accuracy: 0.7916666865348816\n"
     ]
    }
   ],
   "source": [
    "#多尺度\n",
    "model=MDB_class(1,10,4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=1e-4)\n",
    "for j in range(15):\n",
    "    train_accuracy=0\n",
    "    model.train()\n",
    "    for train,labels in train_loader:\n",
    "        out=model(train,10)\n",
    "        pred=torch.argmax(out,dim=1)\n",
    "        loss = loss_fn(out,labels)\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        train_accuracy+=(pred== labels).sum()\n",
    "    print(\" train set  accuracy: {}\".format(train_accuracy/len(train_indices)))\n",
    "    model.eval()\n",
    "    total_test_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for test ,labels  in validation_loader: \n",
    "            outputs = model(test,10)\n",
    "            pred=torch.argmax(outputs,dim=1)\n",
    "            loss = loss_fn(outputs,labels)\n",
    "            total_test_loss = total_test_loss + loss.item()\n",
    "            accuracy = (pred == labels).sum()\n",
    "            total_accuracy = total_accuracy + accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ebf9cfd872009544a161647ac82c48f4cc096aba58631b69e515c7576d66293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
