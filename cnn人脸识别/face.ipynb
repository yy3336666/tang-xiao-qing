{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "from PIL import Image, ImageTk\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import  Reshape\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    if image.ndim == 2:  # 如果图像已经是灰度图像\n",
    "        gray = image\n",
    "    else:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, (224, 224))\n",
    "    normalized = resized / 255.0\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lfw_dataset(data_path, image_size=(224,224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    unique_labels = {}\n",
    "\n",
    "    for person in os.listdir(data_path):\n",
    "        person_dir = os.path.join(data_path, person)\n",
    "        if os.path.isdir(person_dir):\n",
    "            for image_file in os.listdir(person_dir):\n",
    "                image_path = os.path.join(person_dir, image_file)\n",
    "                image = cv2.imread(image_path)\n",
    "                image = preprocess_image(image)\n",
    "                image = cv2.resize(image, image_size)\n",
    "                images.append(image)\n",
    "\n",
    "                # 分配一个数字标签给每个人物\n",
    "                if person not in unique_labels:\n",
    "                    unique_labels[person] = len(unique_labels)\n",
    "                labels.append(unique_labels[person])\n",
    "\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    return images, labels\n",
    "\n",
    "lfw_data_path = './data/face'\n",
    "images, labels = load_lfw_dataset(lfw_data_path)\n",
    "images = images[..., np.newaxis]  # 将图像维度从 (H, W) 扩展为 (H, W, 1)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(images, labels, test_size=0.25, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=len(np.unique(labels))\n",
    "model=create_model(num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "69/69 [==============================] - 13s 181ms/step - loss: 3.6554 - accuracy: 0.0436 - val_loss: 3.6109 - val_accuracy: 0.0574\n",
      "Epoch 2/30\n",
      "69/69 [==============================] - 13s 188ms/step - loss: 3.4761 - accuracy: 0.0749 - val_loss: 3.3293 - val_accuracy: 0.1032\n",
      "Epoch 3/30\n",
      "69/69 [==============================] - 12s 173ms/step - loss: 3.2154 - accuracy: 0.1251 - val_loss: 3.0595 - val_accuracy: 0.1511\n",
      "Epoch 4/30\n",
      "69/69 [==============================] - 12s 174ms/step - loss: 3.0053 - accuracy: 0.1639 - val_loss: 2.8184 - val_accuracy: 0.2058\n",
      "Epoch 5/30\n",
      "69/69 [==============================] - 12s 173ms/step - loss: 2.8199 - accuracy: 0.2063 - val_loss: 2.4925 - val_accuracy: 0.2713\n",
      "Epoch 6/30\n",
      "69/69 [==============================] - 12s 173ms/step - loss: 2.7128 - accuracy: 0.2296 - val_loss: 2.4488 - val_accuracy: 0.2895\n",
      "Epoch 7/30\n",
      "69/69 [==============================] - 12s 172ms/step - loss: 2.5303 - accuracy: 0.2773 - val_loss: 2.1947 - val_accuracy: 0.3576\n",
      "Epoch 8/30\n",
      "69/69 [==============================] - 12s 175ms/step - loss: 2.3823 - accuracy: 0.3086 - val_loss: 2.0895 - val_accuracy: 0.3974\n",
      "Epoch 9/30\n",
      "69/69 [==============================] - 12s 172ms/step - loss: 2.2663 - accuracy: 0.3442 - val_loss: 1.9964 - val_accuracy: 0.4136\n",
      "Epoch 10/30\n",
      "69/69 [==============================] - 12s 178ms/step - loss: 2.1741 - accuracy: 0.3716 - val_loss: 1.7685 - val_accuracy: 0.4892\n",
      "Epoch 11/30\n",
      "69/69 [==============================] - 13s 184ms/step - loss: 2.0730 - accuracy: 0.3953 - val_loss: 1.7854 - val_accuracy: 0.4676\n",
      "Epoch 12/30\n",
      "69/69 [==============================] - 13s 185ms/step - loss: 1.9483 - accuracy: 0.4305 - val_loss: 1.6236 - val_accuracy: 0.5148\n",
      "Epoch 13/30\n",
      "69/69 [==============================] - 13s 186ms/step - loss: 1.8143 - accuracy: 0.4718 - val_loss: 1.4375 - val_accuracy: 0.5789\n",
      "Epoch 14/30\n",
      "69/69 [==============================] - 13s 188ms/step - loss: 1.7709 - accuracy: 0.4736 - val_loss: 1.4312 - val_accuracy: 0.5776\n",
      "Epoch 15/30\n",
      "69/69 [==============================] - 12s 178ms/step - loss: 1.6113 - accuracy: 0.5282 - val_loss: 1.3884 - val_accuracy: 0.5816\n",
      "Epoch 16/30\n",
      "69/69 [==============================] - 14s 203ms/step - loss: 1.5672 - accuracy: 0.5350 - val_loss: 1.3486 - val_accuracy: 0.5897\n",
      "Epoch 17/30\n",
      "69/69 [==============================] - 13s 180ms/step - loss: 1.5139 - accuracy: 0.5563 - val_loss: 1.1705 - val_accuracy: 0.6511\n",
      "Epoch 18/30\n",
      "69/69 [==============================] - 13s 186ms/step - loss: 1.4396 - accuracy: 0.5732 - val_loss: 1.2285 - val_accuracy: 0.6350\n",
      "Epoch 19/30\n",
      "69/69 [==============================] - 12s 180ms/step - loss: 1.4146 - accuracy: 0.5807 - val_loss: 1.1289 - val_accuracy: 0.6559\n",
      "Epoch 20/30\n",
      "69/69 [==============================] - 13s 181ms/step - loss: 1.3170 - accuracy: 0.6110 - val_loss: 1.0257 - val_accuracy: 0.7011\n",
      "Epoch 21/30\n",
      "69/69 [==============================] - 13s 185ms/step - loss: 1.2413 - accuracy: 0.6323 - val_loss: 0.9625 - val_accuracy: 0.7314\n",
      "Epoch 22/30\n",
      "69/69 [==============================] - 12s 177ms/step - loss: 1.1721 - accuracy: 0.6542 - val_loss: 0.9409 - val_accuracy: 0.7220\n",
      "Epoch 23/30\n",
      "69/69 [==============================] - 12s 172ms/step - loss: 1.1647 - accuracy: 0.6546 - val_loss: 0.9499 - val_accuracy: 0.7078\n",
      "Epoch 24/30\n",
      "69/69 [==============================] - 12s 174ms/step - loss: 1.1128 - accuracy: 0.6677 - val_loss: 0.8600 - val_accuracy: 0.7463\n",
      "Epoch 25/30\n",
      "69/69 [==============================] - 13s 182ms/step - loss: 1.0550 - accuracy: 0.6809 - val_loss: 0.7797 - val_accuracy: 0.7584\n",
      "Epoch 26/30\n",
      "69/69 [==============================] - 12s 178ms/step - loss: 1.0180 - accuracy: 0.6912 - val_loss: 0.8273 - val_accuracy: 0.7625\n",
      "Epoch 27/30\n",
      "69/69 [==============================] - 12s 171ms/step - loss: 1.0268 - accuracy: 0.6973 - val_loss: 0.7604 - val_accuracy: 0.7807\n",
      "Epoch 28/30\n",
      "69/69 [==============================] - 12s 173ms/step - loss: 0.9637 - accuracy: 0.7154 - val_loss: 0.7951 - val_accuracy: 0.7578\n",
      "Epoch 29/30\n",
      "69/69 [==============================] - 13s 192ms/step - loss: 0.9559 - accuracy: 0.7119 - val_loss: 0.7234 - val_accuracy: 0.7969\n",
      "Epoch 30/30\n",
      "69/69 [==============================] - 12s 173ms/step - loss: 0.9205 - accuracy: 0.7243 - val_loss: 0.7080 - val_accuracy: 0.7969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e46a1783d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "model.fit(train_images, train_labels, steps_per_epoch=len(train_images) // batch_size, epochs=epochs, validation_data=(val_images,  val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('face_recognition_model.h5')\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "loaded_model = tf.keras.models.load_model('face_recognition_model.h5')\n",
    "def recognize_faces(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=6, minSize=(20, 20))\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = gray[y:y+h, x:x+w]\n",
    "        face_roi = cv2.resize(face_roi, (224, 224))\n",
    "        face_roi = preprocess_image(face_roi)\n",
    "        face_roi = np.expand_dims(face_roi, axis=-1)\n",
    "        face_roi = np.expand_dims(face_roi, axis=0)\n",
    "\n",
    "        predictions = loaded_model.predict(face_roi)\n",
    "        predicted_label = np.argmax(predictions)\n",
    "\n",
    "        # 在原始图像上绘制检测到的人脸矩形和识别结果\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(image, f\"Person {predicted_label}\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 222, 222, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 109, 109, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 52, 52, 64)        36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 173056)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               22151296  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                2600      \n",
      "=================================================================\n",
      "Total params: 22,217,896\n",
      "Trainable params: 22,217,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def load_image(label):\n",
    "    global image_path\n",
    "    image_path = filedialog.askopenfilename()\n",
    "    image = Image.open(image_path)\n",
    "    image.thumbnail((500, 500))\n",
    "    image = ImageTk.PhotoImage(image)\n",
    "    label.config(image=image)\n",
    "    label.image = image\n",
    "\n",
    "def process_image(label):\n",
    "    image = cv2.imread(image_path)\n",
    "    output_image = recognize_faces(image)\n",
    "    output_image = cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB)\n",
    "    output_image = Image.fromarray(output_image)\n",
    "    output_image.thumbnail((500, 500))\n",
    "    output_image = ImageTk.PhotoImage(output_image)\n",
    "    label.config(image=output_image)\n",
    "    label.image = output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[440 144  87  87]]\n",
      "(1, 40)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 创建Tkinter窗口\n",
    "root = Tk()\n",
    "root.title(\"Face Recognition\")\n",
    "\n",
    "# 添加画布和标签显示图像\n",
    "canvas = Canvas(root)\n",
    "canvas.pack()\n",
    "label = Label(root)\n",
    "label.pack()\n",
    "\n",
    "button=Button(root,text=\"Open Image\", command=lambda:load_image(label))\n",
    "button.pack()\n",
    "\n",
    "# 添加识别按钮\n",
    "button = Button(root, text=\"Recognize Faces\", command=lambda:process_image(label))\n",
    "button.pack()\n",
    "\n",
    "# 运行Tkinter主循环\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
