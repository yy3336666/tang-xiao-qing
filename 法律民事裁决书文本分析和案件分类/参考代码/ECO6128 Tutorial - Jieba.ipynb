{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03b8a7e",
   "metadata": {},
   "source": [
    "# ECO6128 Tutorial - Jieba\n",
    "\n",
    "\"Jieba\" (Chinese for \"to stutter\") Chinese text segmentation: built to be the best Python Chinese word segmentation module.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Support three types of segmentation mode:\n",
    "\n",
    "1. Accurate Mode attempts to cut the sentence into the most accurate segmentations, which is suitable for text analysis.\n",
    "\n",
    "2. Full Mode gets all the possible words from the sentence. Fast but not accurate.\n",
    "\n",
    "3. Search Engine Mode, based on the Accurate Mode, attempts to cut long words into several short words, which can raise the recall rate. Suitable for search engines.\n",
    "\n",
    "- Supports Traditional Chinese\n",
    "\n",
    "- Supports customized dictionaries\n",
    "\n",
    "- MIT License\n",
    "\n",
    "Created by *Xinghao YU*, March 18th, 2023, for more please refer to [JIEBA](https://github.com/fxsjy/jieba/blob/master/README.md)\n",
    "\n",
    "*Copyright@Chinese University of Hong Kong, Shenzhen*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59de3dfb",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28f58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anaconda Prompt\n",
    "!conda install jieba\n",
    "\n",
    "# Default Terminal Prompt\n",
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ccfe5",
   "metadata": {},
   "source": [
    "## Segmentation\n",
    "\n",
    "### Introduction of CUT function\n",
    "\n",
    "*cut(sentence, cut_all=False, HMM=True)* -- return generator\n",
    "\n",
    "*lcut(sentence)* -- return word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddebaabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "sentence = '我爱自然语言处理'\n",
    "\n",
    "# 创建[Tokenizer.cut 生成器]对象\n",
    "generator = jieba.cut(sentence)\n",
    "\n",
    "# 遍历生成器，打印分词结果\n",
    "words = '/'.join(generator)\n",
    "print(words)\n",
    "\n",
    "# 打印列表\n",
    "print(jieba.lcut('我爱香港中文大学（深圳）'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d0db4a",
   "metadata": {},
   "source": [
    "### Different modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e1818",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '订单数据分析'\n",
    "\n",
    "print('精准模式：', jieba.lcut(sentence))\n",
    "print('全模式：', jieba.lcut(sentence, cut_all=True))\n",
    "print('搜索引擎模式：', jieba.lcut_for_search(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43557f5b",
   "metadata": {},
   "source": [
    "### Part of Speech Tagging\n",
    "\n",
    "*jieba.posseg.POSTokenizer(tokenizer=None)* creates a new customized Tokenizer. *tokenizer* specifies the *jieba.Tokenizer* to internally use. *jieba.posseg.dt* is the default POSTokenizer. Tags the POS of each word after segmentation, using labels compatible with ictclas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a8397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba.posseg as pseg\n",
    "\n",
    "sentence = '我爱Python数据分析'\n",
    "posseg = pseg.cut(sentence)\n",
    "for i in posseg:\n",
    "    print(i.__dict__)\n",
    "    print(i.word, i.flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29498e7e",
   "metadata": {},
   "source": [
    "### Tokenize: return words with position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = jieba.tokenize('永和服装饰品有限公司')\n",
    "\n",
    "for tk in result:\n",
    "    print(\"word %s\\t start: %d \\t end:%d\" % (tk[0],tk[1],tk[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4800d",
   "metadata": {},
   "source": [
    "## Modify Dictionary\n",
    "\n",
    "add word to the default dictionary, \n",
    "    \n",
    "    add_word(word, freq=None, tag=None)\n",
    "\n",
    "remove specific word from the dictionary, equals to *add_word(word, freq=0)*\n",
    "    \n",
    "    del_word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2115fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '天长地久有时尽，此恨绵绵无绝期'\n",
    "\n",
    "# add\n",
    "jieba.add_word('时尽', 999, 'nz')\n",
    "print('添加[时尽]：', jieba.lcut(sentence))\n",
    "\n",
    "# remove\n",
    "jieba.del_word('时尽')\n",
    "print('删除[时尽]：', jieba.lcut(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5c3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
