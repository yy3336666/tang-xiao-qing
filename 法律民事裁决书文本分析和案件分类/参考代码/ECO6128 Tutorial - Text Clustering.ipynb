{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECO6128 Tutorial - Text Clustering\n",
    "\n",
    "Introducing *k*-means and DBSCAN clustering. For more, please refer to [scikit demo](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py)\n",
    "\n",
    "Created by *Xinghao YU*, March 18th, 2023\n",
    "\n",
    "*Copyright@Chinese University of Hong Kong, Shenzhen*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-19T08:53:45.060142Z",
     "start_time": "2023-03-19T08:53:45.053805Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *k*-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-19T08:45:34.081604Z",
     "start_time": "2023-03-19T08:45:33.790310Z"
    }
   },
   "outputs": [],
   "source": [
    "class KmeansClustering():\n",
    "    def __init__(self, stopwords_path=None):\n",
    "        self.stopwords = self.load_stopwords(stopwords_path)\n",
    "        self.vectorizer = CountVectorizer()\n",
    "        self.transformer = TfidfTransformer()\n",
    "\n",
    "    def load_stopwords(self, stopwords=None):\n",
    "        \"\"\"\n",
    "        加载停用词\n",
    "        :param stopwords:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if stopwords:\n",
    "            with open(stopwords) as f:\n",
    "                return [line.strip() for line in f]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def preprocess_data(self, corpus_path):\n",
    "        \"\"\"\n",
    "        文本预处理，每行一个文本\n",
    "        :param corpus_path:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                corpus.append(' '.join([word for word in jieba.lcut(line.strip()) if word not in self.stopwords]))\n",
    "        return corpus\n",
    "\n",
    "    def get_text_tfidf_matrix(self, corpus):\n",
    "        \"\"\"\n",
    "        获取tfidf矩阵\n",
    "        :param corpus:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tfidf = self.transformer.fit_transform(self.vectorizer.fit_transform(corpus))\n",
    "\n",
    "        # 获取词袋中所有词语\n",
    "        # words = self.vectorizer.get_feature_names()\n",
    "\n",
    "        # 获取tfidf矩阵中权重\n",
    "        weights = tfidf.toarray()\n",
    "        return weights\n",
    "\n",
    "    def kmeans(self, corpus_path, n_clusters=5):\n",
    "        \"\"\"\n",
    "        KMeans文本聚类\n",
    "        :param corpus_path: 语料路径（每行一篇）,文章id从0开始\n",
    "        :param n_clusters: ：聚类类别数目\n",
    "        :return: {cluster_id1:[text_id1, text_id2]}\n",
    "        \"\"\"\n",
    "        corpus = self.preprocess_data(corpus_path)\n",
    "        weights = self.get_text_tfidf_matrix(corpus)\n",
    "\n",
    "        clf = KMeans(n_clusters=n_clusters)\n",
    "\n",
    "        # clf.fit(weights)\n",
    "\n",
    "        y = clf.fit_predict(weights)\n",
    "\n",
    "        # 中心点\n",
    "        # centers = clf.cluster_centers_\n",
    "\n",
    "        # 用来评估簇的个数是否合适,距离约小说明簇分得越好,选取临界点的簇的个数\n",
    "        # score = clf.inertia_\n",
    "\n",
    "        # 每个样本所属的簇\n",
    "        result = {}\n",
    "        for text_idx, label_idx in enumerate(y):\n",
    "            if label_idx not in result:\n",
    "                result[label_idx] = [text_idx]\n",
    "            else:\n",
    "                result[label_idx].append(text_idx)\n",
    "        return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Kmeans = KmeansClustering(stopwords_path=r'./stop_word.txt')\n",
    "    result = Kmeans.kmeans(r'./hotel.txt', n_clusters=3)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-19T08:46:35.233648Z",
     "start_time": "2023-03-19T08:46:34.889251Z"
    }
   },
   "outputs": [],
   "source": [
    "class DbscanClustering():\n",
    "    def __init__(self, stopwords_path=None):\n",
    "        self.stopwords = self.load_stopwords(stopwords_path)\n",
    "        self.vectorizer = CountVectorizer()\n",
    "        self.transformer = TfidfTransformer()\n",
    "\n",
    "    def load_stopwords(self, stopwords=None):\n",
    "        \"\"\"\n",
    "        加载停用词\n",
    "        :param stopwords:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if stopwords:\n",
    "            with open(stopwords, 'r', encoding='utf-8') as f:\n",
    "                return [line.strip() for line in f]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def preprocess_data(self, corpus_path):\n",
    "        \"\"\"\n",
    "        文本预处理，每行一个文本\n",
    "        :param corpus_path:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        corpus = []\n",
    "        with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                corpus.append(' '.join([word for word in jieba.lcut(line.strip()) if word not in self.stopwords]))\n",
    "        return corpus\n",
    "\n",
    "    def get_text_tfidf_matrix(self, corpus):\n",
    "        \"\"\"\n",
    "        获取tfidf矩阵\n",
    "        :param corpus:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tfidf = self.transformer.fit_transform(self.vectorizer.fit_transform(corpus))\n",
    "\n",
    "        # 获取词袋中所有词语\n",
    "        # words = self.vectorizer.get_feature_names()\n",
    "\n",
    "        # 获取tfidf矩阵中权重\n",
    "        weights = tfidf.toarray()\n",
    "        return weights\n",
    "\n",
    "    def pca(self, weights, n_components=3):\n",
    "        \"\"\"\n",
    "        PCA对数据进行降维\n",
    "        :param weights:\n",
    "        :param n_components:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pca = PCA(n_components=n_components)\n",
    "        return pca.fit_transform(weights)\n",
    "\n",
    "    def dbscan(self, corpus_path, eps=0.1, min_samples=3, fig=True):\n",
    "        \"\"\"\n",
    "        DBSCAN：基于密度的文本聚类算法\n",
    "        :param corpus_path: 语料路径，每行一个文本\n",
    "        :param eps: DBSCA中半径参数\n",
    "        :param min_samples: DBSCAN中半径eps内最小样本数目\n",
    "        :param fig: 是否对降维后的样本进行画图显示\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        corpus = self.preprocess_data(corpus_path)\n",
    "        weights = self.get_text_tfidf_matrix(corpus)\n",
    "\n",
    "        pca_weights = self.pca(weights)\n",
    "\n",
    "        clf = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')\n",
    "\n",
    "        y = clf.fit_predict(pca_weights)\n",
    "\n",
    "        if fig:\n",
    "            plt.scatter(pca_weights[:, 0], pca_weights[:, 1], c=y)\n",
    "            plt.show()\n",
    "\n",
    "        # 中心点\n",
    "        # centers = clf.cluster_centers_\n",
    "\n",
    "        # 用来评估簇的个数是否合适,距离约小说明簇分得越好,选取临界点的簇的个数\n",
    "        # score = clf.inertia_\n",
    "\n",
    "        # 每个样本所属的簇\n",
    "        result = {}\n",
    "        for text_idx, label_idx in enumerate(y):\n",
    "            if label_idx not in result:\n",
    "                result[label_idx] = [text_idx]\n",
    "            else:\n",
    "                result[label_idx].append(text_idx)\n",
    "        return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dbscan = DbscanClustering(stopwords_path=r'./stop_word.txt')\n",
    "    result = dbscan.dbscan(r'./hotel.txt', eps=0.2, min_samples=5)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
