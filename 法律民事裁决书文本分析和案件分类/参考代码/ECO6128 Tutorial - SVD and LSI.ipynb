{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECO6128 Tutorial - SVD and LSI\n",
    "\n",
    "*Latent Semantic Indexing (LSI)* is a method for discovering hidden concepts in document data. Each document and term (word) is then expressed as a vector with elements corresponding to these concepts. Each element in a vector gives the degree of participation of the document or term in the corresponding concept. The goal is not to describe the concepts verbally, but to be able to represent the documents and terms in a unified way for exposing document-document, document-term, and term-term similarities or semantic relationship which are otherwise hidden.\n",
    "\n",
    "Created by *Xinghao YU*, March 18th, 2023. For more, please refer to [./Refer - SVD Tutorial (Alex Thomo).pdf]\n",
    "\n",
    "*Copyright@Chinese University of Hong Kong, Shenzhen*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct sentences\n",
    "corpus = ['Romeo Juliet.',\n",
    "          'Juliet happy dagger!',\n",
    "          'Romeo die dagger.',\n",
    "          '“Live free die”, that’s the Hampshire’s',\n",
    "          'Hampshire is in.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw documents to tf matrix; not normalized; not use idf\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             norm=None,\n",
    "                             use_idf=False)\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the key terms\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the sparse matrix\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD to reduce dimensionality: here we choose only 2 concepts\n",
    "svd_model = TruncatedSVD(n_components=2,       \n",
    "                         algorithm='randomized',\n",
    "                         n_iter=5)\n",
    "dc_matrix = svd_model.fit_transform(X)\n",
    "# output: the scaled document-concept matrix\n",
    "document_concept_matrix = pd.DataFrame(dc_matrix)\n",
    "\n",
    "d = []\n",
    "for row in range(0, document_concept_matrix.shape[0]):\n",
    "    d.append(f'd{row+1}')\n",
    "document_concept_matrix.index = d\n",
    "document_concept_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $.components_ return 'The right singular vectors of the input data', that is concept-term matrix\n",
    "# $.singular_values_ return 'The singular values corresponding to each of the selected components'\n",
    "# What we need: the scaled term-concept matrix\n",
    "tc_matrix = np.dot(svd_model.components_.T, np.diag(svd_model.singular_values_))\n",
    "term_concept_matrix = pd.DataFrame(tc_matrix)\n",
    "term_concept_matrix.index = vectorizer.get_feature_names_out()\n",
    "term_concept_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all vectors\n",
    "document_term = pd.concat([document_concept_matrix, term_concept_matrix])\n",
    "plt.scatter(x = document_term[0], y = document_term[1])\n",
    "# add labels to all points\n",
    "for idx, row in document_term.iterrows(): \n",
    "    plt.text(row[0], row[1], idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about we use tf-idf, with normalization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['Romeo and Juliet.',\n",
    "          'Juliet: O happy dagger!',\n",
    "          'Romeo died by dagger.',\n",
    "          '“Live free or die”, that’s the New-Hampshire’s motto.',\n",
    "          'Did you know, New-Hampshire is in New-England.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw documents to tf-idf matrix: \n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             use_idf=True, \n",
    "                             smooth_idf=True)\n",
    "# SVD to reduce dimensionality: \n",
    "svd_model = TruncatedSVD(n_components=2,       \n",
    "                         algorithm='randomized',\n",
    "                         n_iter=5)\n",
    "# pipeline of tf-idf + SVD, fit to and applied to documents:\n",
    "svd_transformer = Pipeline([('tfidf', vectorizer), \n",
    "                            ('svd', svd_model)])\n",
    "dc_matrix = svd_transformer.fit_transform(corpus)\n",
    "# dc_matrix can later be used to compare documents, compare words, or compare queries with documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_concept_matrix = pd.DataFrame(dc_matrix)\n",
    "\n",
    "d = []\n",
    "for row in range(0, document_concept_matrix.shape[0]):\n",
    "    d.append(f'd{row+1}')\n",
    "document_concept_matrix.index = d\n",
    "\n",
    "tc_matrix = np.dot(svd_model.components_.T, np.diag(svd_model.singular_values_))\n",
    "term_concept_matrix = pd.DataFrame(tc_matrix)\n",
    "term_concept_matrix.index = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_term = pd.concat([document_concept_matrix, term_concept_matrix])\n",
    "document_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all vectors\n",
    "plt.scatter(x = document_term[0], y = document_term[1])\n",
    "# add labels to all points\n",
    "for idx, row in document_term.iterrows(): \n",
    "    plt.text(row[0], row[1], idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
